# -*- coding: utf-8 -*-
"""MLAssignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1arr5bittpwBs3JRwl2tT4OVHMF4lZktL
"""

!pip install torch torchvision wandb pillow numpy torchmetrics==0.11.4

import wandb
wandb.login()

from google.colab import files
import os
import zipfile

!rm -rf /content/dataset

uploaded = files.upload()

zip_file = next((f for f in uploaded.keys() if f.endswith('.zip')), None)
if zip_file:
    with zipfile.ZipFile(zip_file, 'r') as zip_ref:
        zip_ref.extractall(".")
    print("Top-level directories:", os.listdir("."))
    if os.path.exists("dataset"):
        print("Dataset structure after unzip:", os.listdir("dataset"))
        nested_dir = os.path.join("dataset", "dataset")
        if os.path.exists(nested_dir):
            print(f"Nested dataset folder found at {nested_dir}. Using this as root_dir.")
            dataset_root = nested_dir
            print("Nested dataset contents:", os.listdir(nested_dir))
        else:
            dataset_root = "dataset"
        training_dir = os.path.join(dataset_root, "training")
        if os.path.exists(training_dir):
            print("Training directory contents:", os.listdir(training_dir))
            if os.path.exists(os.path.join(training_dir, "data.json")):
                print("Found training/data.json")
            else:
                print("Error: training/data.json not found")
        else:
            print(f"Error: {training_dir} not found")
    else:
        raise RuntimeError("Failed to unzip dataset. Directory ./dataset not created.")
else:
    raise FileNotFoundError("No .zip file uploaded")

import json
import os
from PIL import Image
import torch
from torch.utils.data import Dataset
import torchvision.transforms as transforms
import numpy as np

class PolygonDataset(Dataset):
    def __init__(self, root_dir, split='training', transform=None, max_colors=8):
        self.root_dir = root_dir
        self.split = split
        self.transform = transform
        self.max_colors = max_colors
        self.input_dir = os.path.join(root_dir, split, 'inputs')
        self.output_dir = os.path.join(root_dir, split, 'outputs')

        json_path = os.path.join(root_dir, split, 'data.json')
        if not os.path.exists(json_path):
            print(f"Error: {json_path} not found")
            print(f"Listing contents of {root_dir}:")
            os.system(f"ls -R {root_dir}")
            print("Please verify the dataset structure. Expected: training/inputs, training/outputs, training/data.json")
            raise FileNotFoundError(f"{json_path} not found")
        with open(json_path, 'r') as f:
            self.data = json.load(f)

        if self.data:
            print(f"Sample data.json entry for {split}:", self.data[0])

        color_key = None
        possible_keys = ['color', 'colour', 'label', 'class']
        for key in possible_keys:
            if key in self.data[0]:
                color_key = key
                break
        if color_key is None:
            raise KeyError(f"No valid color key found in data.json. Tried: {possible_keys}. Sample entry: {self.data[0]}")

        self.color_map = {}
        for i, entry in enumerate(self.data):
            color = entry[color_key]
            if color not in self.color_map:
                self.color_map[color] = len(self.color_map)
        self.num_colors = len(self.color_map)
        print(f"Color map for {split}: {self.color_map}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        entry = self.data[idx]
        color_key = 'colour'
        for key in ['color', 'colour', 'label', 'class']:
            if key in entry:
                color_key = key
                break

        input_img_path = os.path.join(self.input_dir, entry['input_polygon'])
        output_img_path = os.path.join(self.output_dir, entry['output_image'])
        color_name = entry[color_key]


        try:
            input_img = Image.open(input_img_path).convert('L')
            output_img = Image.open(output_img_path).convert('RGB')
        except Exception as e:
            raise RuntimeError(f"Error loading images for index {idx}: {e}")

        if self.transform:
            seed = np.random.randint(2147483647)
            torch.manual_seed(seed)
            input_img = self.transform(input_img)
            torch.manual_seed(seed)
            output_img = self.transform(output_img)

        color_idx = self.color_map[color_name]
        color_vector = torch.zeros(self.max_colors)
        color_vector[color_idx] = 1.0

        return input_img, color_vector, output_img

transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.RandomRotation(30),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor()
])

import torch
import torch.nn as nn

class UNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=3, num_colors=8):
        super(UNet, self).__init__()
        self.num_colors = num_colors

        def conv_block(in_ch, out_ch):
            block = nn.Sequential(
                nn.Conv2d(in_ch, out_ch, 3, padding=1),
                nn.BatchNorm2d(out_ch),
                nn.ReLU(inplace=True),
                nn.Conv2d(out_ch, out_ch, 3, padding=1),
                nn.BatchNorm2d(out_ch),
                nn.ReLU(inplace=True)
            )
            for m in block.modules():
                if isinstance(m, nn.Conv2d):
                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
            return block

        self.enc1 = conv_block(in_channels + num_colors, 64)
        self.enc2 = conv_block(64, 128)
        self.enc3 = conv_block(128, 256)
        self.enc4 = conv_block(256, 512)
        self.pool = nn.MaxPool2d(2, 2)

        self.bottleneck = conv_block(512, 1024)

        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)
        self.dec4 = conv_block(1024, 512)
        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = conv_block(512, 256)
        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = conv_block(256, 128)
        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = conv_block(128, 64)
        self.final = nn.Conv2d(64, out_channels, 1)

    def forward(self, x, color_vector):
        color_vector = color_vector[:, :, None, None].expand(-1, -1, x.size(2), x.size(3))
        x = torch.cat([x, color_vector], dim=1)

        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        e4 = self.enc4(self.pool(e3))

        b = self.bottleneck(self.pool(e4))

        d4 = self.up4(b)
        d4 = torch.cat([d4, e4], dim=1)
        d4 = self.dec4(d4)
        d3 = self.up3(d4)
        d3 = torch.cat([d3, e3], dim=1)
        d3 = self.dec3(d3)
        d2 = self.up2(d3)
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2(d2)
        d1 = self.up1(d2)
        d1 = torch.cat([d1, e1], dim=1)
        d1 = self.dec1(d1)
        out = self.final(d1)
        return torch.sigmoid(out)

import numpy as np

try:
    from torchmetrics.image import StructuralSimilarityIndexMeasure
    ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
    use_torchmetrics = True
except ModuleNotFoundError:
    print("torchmetrics not found, using custom SSIM implementation")
    use_torchmetrics = False
    def custom_ssim(img1, img2, data_range=1.0, win_size=11):
        img1 = img1.cpu().numpy().transpose(0, 2, 3, 1)
        img2 = img2.cpu().numpy().transpose(0, 2, 3, 1)
        batch_size = img1.shape[0]
        ssim_scores = []

        for i in range(batch_size):
            mu1 = np.mean(img1[i])
            mu2 = np.mean(img2[i])
            sigma1 = np.std(img1[i])
            sigma2 = np.std(img2[i])
            sigma12 = np.mean((img1[i] - mu1) * (img2[i] - mu2))
            c1 = (0.01 * data_range) ** 2
            c2 = (0.03 * data_range) ** 2
            ssim_score = ((2 * mu1 * mu2 + c1) * (2 * sigma12 + c2)) / ((mu1**2 + mu2**2 + c1) * (sigma1**2 + sigma2**2 + c2))
            ssim_scores.append(ssim_score)

        return np.mean(ssim_scores)

from torch.utils.data import DataLoader
import wandb

wandb.init(project="ayna_ml_assignment", config={
    "learning_rate": 1e-3,
    "epochs": 50,
    "batch_size": 16,
    "architecture": "UNet",
    "input_channels": 1,
    "output_channels": 3,
    "optimizer": "Adam",
    "loss_function": "MSELoss",
    "image_size": (128, 128),
    "augmentations": ["Resize", "RandomRotation", "RandomHorizontalFlip"]
})

train_dataset = PolygonDataset(dataset_root, split='training', transform=transform)
val_dataset = PolygonDataset(dataset_root, split='validation', transform=transform)
train_loader = DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=wandb.config.batch_size)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = UNet(num_colors=len(train_dataset.color_map)).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)

best_val_loss = float('inf')
best_model_path = 'best_unet_model.pth'

for epoch in range(wandb.config.epochs):
    model.train()
    train_loss = 0
    train_ssim = 0
    for input_img, color_vector, output_img in train_loader:
        input_img, color_vector, output_img = input_img.to(device), color_vector.to(device), output_img.to(device)
        optimizer.zero_grad()
        pred = model(input_img, color_vector)
        loss = criterion(pred, output_img)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
        if use_torchmetrics:
            train_ssim += ssim(pred, output_img).item()
        else:
            train_ssim += custom_ssim(pred, output_img)

    train_loss /= len(train_loader)
    train_ssim /= len(train_loader)

    model.eval()
    val_loss = 0
    val_ssim = 0
    with torch.no_grad():
        for input_img, color_vector, output_img in val_loader:
            input_img, color_vector, output_img = input_img.to(device), color_vector.to(device), output_img.to(device)
            pred = model(input_img, color_vector)
            val_loss += criterion(pred, output_img).item()
            if use_torchmetrics:
                val_ssim += ssim(pred, output_img).item()
            else:
                val_ssim += custom_ssim(pred, output_img)

    val_loss /= len(val_loader)
    val_ssim /= len(val_loader)

    scheduler.step(val_loss)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), best_model_path)
        wandb.save(best_model_path)

    wandb.log({
        'epoch': epoch,
        'train_loss': train_loss,
        'val_loss': val_loss,
        'train_ssim': train_ssim,
        'val_ssim': val_ssim,
        'learning_rate': optimizer.param_groups[0]['lr']
    })

    if epoch % 10 == 0:
        sample_input = input_img[0].cpu().numpy().transpose(1, 2, 0)
        sample_pred = pred[0].cpu().numpy().transpose(1, 2, 0)
        sample_gt = output_img[0].cpu().numpy().transpose(1, 2, 0)
        wandb.log({
            'sample': [
                wandb.Image(sample_input, caption="Input"),
                wandb.Image(sample_pred, caption="Predicted"),
                wandb.Image(sample_gt, caption="Ground Truth")
            ]
        })

    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train SSIM: {train_ssim:.4f}, Val SSIM: {val_ssim:.4f}')


torch.save(model.state_dict(), 'final_unet_model.pth')
wandb.save('final_unet_model.pth')